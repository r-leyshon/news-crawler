name: Daily Article Crawl

on:
  schedule:
    # Run daily at 4:00 AM UTC
    - cron: '0 4 * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements.txt
        
    - name: Run crawl
      env:
        AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
        AZURE_OPENAI_KEY: ${{ secrets.AZURE_OPENAI_KEY }}
      run: |
        cd backend
        python -c "
        import asyncio
        import aiohttp
        import json
        
        async def trigger_crawl():
            async with aiohttp.ClientSession() as session:
                payload = {
                    'keywords': ['artificial intelligence', 'machine learning', 'technology news'],
                    'max_articles': 10
                }
                # Replace with your deployed API URL
                async with session.post('http://your-api-url.com/crawl', json=payload) as response:
                    result = await response.json()
                    print(f'Crawl result: {result}')
        
        asyncio.run(trigger_crawl())
        "
